# CoCoGAN_pytorch_master
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This is a pytorch-implementation for the base ideas of "Patch Generation" in "[COCO-GAN: Generation by Parts via Conditional Coordinating](https://hubert0527.github.io/COCO-GAN/)". 

# Profile
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Well, we did not do an "honest" implementation such as following the network architectures introduced in the paper :smile:. Insteadâ€‹ :joy:, we followed the base ideas "**Patch Generation and Spatial Relationship + Consistency**" and we implement this based on the original `pytorch` project for **StyleGAN**.<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Since of our poor English and expression skills, in the following part, we will describe **HOW we establish the implementation of `CoCo-GAN` step by step with `StyleGAN`** with a mixing of English and Chinese.

# æ¬§å‡ é‡Œå¾—åæ ‡ç³»ç»Ÿ

æˆ‘ä»¬æ ¹æ®é…ç½® `Config/CelebA_128x128_N4M4S64.yaml` ç”»å‡ºåæ ‡ç³»ç»Ÿã€‚

å…¶ä¸­`N4M4` è¡¨ç¤ºä¸€ä¸ª macro patch ç”± `4x4` ä¸ª micro patches ç»„æˆï¼›`S64` è¡¨ç¤ºä¸€ä¸ª macro patch çš„åƒç´ å¤§å°æ˜¯ `64x64`ï¼›`CelebA` è¡¨ç¤ºè®­ç»ƒæ•°æ®é›†ï¼›`128x128` è¡¨ç¤ºæ•´å›¾çš„å¤§å°ã€‚ 

<img src="note/000.png" alt="image-20200202184929191" style="zoom:67%;" />

# å¤ç°

- æˆ‘ä»¬è®­ç»ƒçš„é…ç½®æ˜¯ `Config/CelebA_128x128_N2M2S64.yaml`
- æˆ‘ä»¬ä»ä¸€ä¸ªç°æœ‰çš„ `noise2img` çš„ `GAN` å¼€å§‹ï¼Œä¸ºäº†ä¾¿äºä¸åŒçš„ `config` è®¾ç½®ï¼Œéœ€è¦ `Generator` å’Œ `Discriminator` èƒ½å¤Ÿè‡ªé€‚åº”è¶…å‚æ•°ï¼ˆe.g. `macro patch size`, `micro patch size`, `ratio of macro to micro`, etc.ï¼‰ï¼Œæˆ‘ä»¬é€‰æ‹© `ProGAN`ï¼Œç”±äºæˆ‘ä¹‹å‰å·²ç»éå¸¸ç†Ÿæ‚‰ `StyleGAN`ï¼Œæˆ‘ä»¬åœ¨å®ƒä¸Šé¢ä¸€æ­¥æ­¥æ”¹è¿›ã€‚

1. Make sure the progressive generation on a sample dataset of `CelebA`(807 faces).

   ```sh
   ## Cmd: 
   CUDA_VISIBLE_DEVICES=0 python train.py --loss r1 --sched --mixing datasets/celeba
   ```

   (iteration: 5800, size: 8Ã—8)

   <img src="note\001.png" alt="image-20200202184929191" style="zoom:125%;" />

   æˆ‘ä»¬åªæ˜¯åœ¨ä¸€ä¸ªåªæœ‰ 806 å¼ å›¾åƒçš„ CelebA å­é›†ä¸Šè®­ç»ƒï¼Œé¦–å…ˆæˆ‘ä»¬è¦ç¡®ä¿è¿™ä¸ªæ•°æ®é›†æ˜¯å¯ç”¨çš„ï¼Œå› æ­¤æˆ‘ä»¬ç›´æ¥åœ¨æœ€å¼€å§‹çš„ `StyleGAN` ä¸Šè®­ç»ƒï¼›åŸºäºç»éªŒï¼Œä»ä¸Šå›¾æˆ‘ä»¬å°±å¯ä»¥è®¤ä¸ºåŸºäºè¿™ä¸ªå°æ•°æ®é›†è®­ç»ƒæ—¶å¯è¡Œçš„ã€‚

2. Make sure the direct generation(w/o progressive) for any target size(e.g. 64Ã—64) on the sample dataset.

   ```sh
   ## Cmd: 
   CUDA_VISIBLE_DEVICES=0 python train_nopro.py --loss r1 --sched --mixing datasets/celeba
   ```

   (iteration: 5200, size: 64Ã—64)

   <img src="note\002.png" alt="image-20200202184929191" style="zoom:67%;" />

   ä¸‹é¢æˆ‘ä»¬å°±è¦ä¿è¯å–æ¶ˆ `Progressive training strategy` ï¼Œç›´æ¥ç”Ÿæˆç›®æ ‡ micro patch å¤§å°çš„æ•´å›¾æ˜¯å¦å¯è¡Œï¼Ÿå¦‚ `32x32`ï¼Œ`64x64` ç­‰ã€‚

3. Make sure learning the patch consistency is possible.

   ```sh
   ## Cmd: 
   CUDA_VISIBLE_DEVICES=0 python train_patches_consistency.py --loss r1 --sched --mixing datasets/celeba
   ```

   (iteration: 102200, size: (4, 32, 32, 3)â†’(1, 64, 64, 3))

   <img src="note\004.png" alt="image-20200202184929191" style="zoom:67%;" />

   > ç°åœ¨æˆ‘ä»¬éœ€è¦è¯æ˜ï¼Œ`GAN` å­¦ä¹  `patch generation` å’Œ `patch consistency`(å³å½“æ•´å›¾åˆ†å¤šä¸ª patches åˆæˆæ—¶ï¼Œç›¸é‚» patches ä¹‹é—´çš„å†…å®¹è¡”æ¥æ˜¯å…¼å®¹çš„)ã€‚
   >
   > æˆ‘ä»¬å°†æ•´å›¾å‡åˆ†ä¸º 4 ä¸ª patchesï¼Œå¯¹åº”åæ ‡ä¸ºï¼š
   >
   ```python
   [0][1]
   [2][3]
   '''
   [0] -> (-1.0,  1.0)
   [1] -> ( 1.0,  1.0)
   [2] -> (-1.0, -1.0)
   [3] -> ( 1.0, -1.0)
   '''
   # æˆ‘ä»¬è®© noise çš„ dim ä¸º 512-2ï¼Œåœ¨ 510-dim çš„ style vector åé¢æ‹¼æ¥ä¸Š 2-dim çš„åæ ‡ä¿¡æ¯
   ```
   >
   > `G` è¾“å‡º 4 ä¸ª micro patches æ•´åˆæˆä¸€ä¸ª macro patch åè¾“å…¥ç»™ `D` é‰´åˆ«ã€‚

4. Make sure learning the spatial relationship is also possible.

   ```sh
   # Cmd for training:
   CUDA_VISIBLE_DEVICES=0 python train_spatial_relationship.py --loss r1 --sched --mixing --path datasets/celeba --config=configs/CelebA_128x128_N2M2S64.yaml
   # Cmd for testing:
   CUDA_VISIBLE_DEVICES=0 python test_spatialR.py --config=configs/CelebA_128x128_N2M2S64.yaml
   ```

   (iteration: 128600, size: (4, 32, 32, 3)â†’(1, 64, 64, 3))

   <img src="note\009.png" alt="image-20200202184929191" style="zoom:67%;" />

   ä¸‹é¢æˆ‘ä»¬è¦è¯æ˜ï¼Œ`GAN` å¯ä»¥æ ¹æ®ä¸åŒçš„åæ ‡ä¿¡æ¯ç”Ÿæˆå›¾åƒå¯¹åº”çš„éƒ¨åˆ†è€Œä¸ä»…ä»…æ˜¯ç”Ÿæˆæ•´å›¾ã€‚

   > åŸå›¾ 128x128ï¼Œè¢«åˆ’åˆ†ä¸º 2x2 ä¸ª macro patchesï¼ˆ64x64ï¼‰ï¼›
   >
   > æ¯ä¸ª macro patch è¢«åˆ’åˆ†ä¸º 2x2 ä¸ª micro patchesï¼ˆ32x32ï¼‰;
   >
   > æˆ‘ä»¬å¢åŠ äº† projection discriminator åˆ° `StyleGAN` çš„é‰´åˆ«å™¨ä¸­ï¼Œ`G` å’Œ `D` æ¥æ”¶çš„åˆ†åˆ«æ˜¯ micros å’Œ macro çš„åæ ‡ä½œä¸º conditionï¼›
   >
   > ä½†æˆ‘ä»¬è¿˜æ²¡æœ‰å¢åŠ é¢å¤–çš„ coordinate consistency lossã€‚

   We also do the test to generate a full image.

   | <img src="note\007.png" alt="image-20200202184929191" style="zoom:100%;" /> | <img src="note\008.png" alt="image-20200202184929191" style="zoom:100%;" /> |
   | :----------------------------------------------------------: | :----------------------------------------------------------: |
   | ç›®å‰ä¸ºæ­¢å¯ä»¥ä¿æŒ patch ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œ<br/>ä½†æ˜¯ coord çš„ä½œç”¨è¿˜ä¸å¼ºï¼Œ<br/>å¤§éƒ¨åˆ†æ˜¯ä¸Šå›¾è¿™æ ·æ— æ³•å‡‘æˆè‡ªç„¶äººè„¸çš„ |         å¾ˆéš¾æ‰æ‰¾åˆ°çš„ä¸€ä¸ªæ¯”è¾ƒè‡ªç„¶çš„æ ·æœ¬<br/>çº¦ 1/256          |

   <img src="note\010.png" alt="sample image" style="zoom:100%;"/>

   Thus, the *spatial consistency loss* is necessary.

5. Add *Spatial Consistency Loss*.

   ç°åœ¨æˆ‘ä»¬å¸Œæœ› `GAN` å¯ä»¥å­¦ä¹ åˆ°ç”Ÿæˆçš„ micro patches æˆ– macro patch ä¸å¯¹åº”çš„ coordinate info æ˜¯ä¸¥æ ¼å¯¹åº”çš„ï¼Œå› æ­¤æˆ‘ä»¬åŠ å…¥ `Spatial Consistency Loss`

   ```sh
   # Cmd for training:
   CUDA_VISIBLE_DEVICES=0 python train_coco.py --loss r1 --sched --mixing --path datasets/celeba --config=configs/CelebA_128x128_N2M2S64.yaml
   # Cmd for testing:
   CUDA_VISIBLE_DEVICES=0 python test_coco.py --config=configs/CelebA_128x128_N2M2S64.yaml
   ```
   
   **Trial 1**
   
   In our first trial with `coord_loss_w = 1` and `code_dim = 510 + 2`, the results is not so good but a bit better than the case w/o.
   
   <img src="note\011.png" alt="sample image" style="zoom:100%;"/>
   
   **Trial 2**
   
   Then we increase the weight subject to `coord_loss_w = 10` and `code_dim = 254 + 2`, the results are much better. 
   
   (Ckp: 150000.model)
   
   <img src="note\012.png" alt="sample image" style="zoom:100%;"/>

| <img src="note\000029.png" alt="sample image" style="zoom:100%;"/> | <img src="note\000031.png" alt="sample image" style="zoom:100%;"/> | <img src="note\000056.png" alt="sample image" style="zoom:100%;"/> | <img src="note\000104.png" alt="sample image" style="zoom:100%;"/> | <img src="note\000129.png" alt="sample image" style="zoom:100%;"/> | <img src="note\000142.png" alt="sample image" style="zoom:100%;"/> |
| :-----------------------: | :-----------------------: | :-----------------------: | :-----------------------: | ------------------------- | ------------------------- |
| <img src="note\000165.png" alt="sample image" style="zoom:100%;"/> | <img src="note\000230.png" alt="sample image" style="zoom:100%;"/> | <img src="note\000169.png" alt="sample image" style="zoom:100%;"/> | <img src="note\000219.png" alt="sample image" style="zoom:100%;"/> | <img src="note\000223.png" alt="sample image" style="zoom:100%;"/> | <img src="note\000228.png" alt="sample image" style="zoom:100%;"/> |
| <img src="note\000234.png" alt="sample image" style="zoom:100%;"/> | <img src="note\000244.png" alt="sample image" style="zoom:100%;"/> | <img src="note\000249.png" alt="sample image" style="zoom:100%;"/> | <img src="note\000250.png" alt="sample image" style="zoom:100%;"/> | <img src="note\000098.png" alt="sample image" style="zoom:100%;"/> | <img src="note\000146.png" alt="sample image" style="zoom:100%;"/> |

For further experiments, we will follow continuously. (å¾—å»åšæ¯•è®¾äº†ğŸ˜­)

# References

[1] [CoCo-GAN *The official `tensorflow`-implementation*](https://github.com/hubert0527/COCO-GAN)

[2] [StyleGAN *The official `tensorflow`-implementation*](https://github.com/NVlabs/stylegan)

[3] A `pytorch`-implementation for `StyleGAN` but we cannot find the source now.

is_training: True
data_params:
  # dataset options: "celeba", "celeba-syn-inward", "celeba-syn-outward", "celeba-hq", "lsun", "mp3d",
  dataset: "celeba"
  c_dim: 3
  full_image_size: [128, 128] # [height, width], supports x!=y
  macro_patch_size: [64, 64] # Ideally supports x!=y, but I have never used it
  micro_patch_size: [16, 16] # Ideally supports x!=y, but I have never used it
  num_train_samples: 152599 # i.e., 202599-50000
  num_test_samples: 50000
  coordinate_system: "euclidean"
  
train_params:
  epochs: inf # No need to specify, usually longer better, and eventually saturates
  batch_size: 128
  G_update_period: 1
  D_update_period: 1
  Q_update_period: 0
  beta1: 0.0
  beta2: 0.999
  glr: 0.0001
  dlr: 0.0004
  qlr: 0.0001
  # Extrapolation post-training args
  train_extrap: False
  # Force loading from this ckpt, otherwise, auto detect
  force_load_from_dir: False
  gpu_id: 0

loss_params:
  gp_lambda: 10
  coord_loss_w: 10
  code_loss_w: 0 

model_params:
  z_dim: 128
  spatial_dim: 2
  g_extra_layers: 0 
  d_extra_layers: 0
  ngf_base: 64
  ndf_base: 64
  aux_dim: 128
  init_ngf_mult: 16
  max_ndf_mult: 8
  n_G_res: 4
  n_D_res: 5

log_params:
  exp_name: "CelebA_128x128_N4M4S64"
  log_dir: "./logs/"

  # Use inf to disable
  log_step: 1
  rcd_step: 100
  img_step: 10   # Consumes quite much disk space
  fid_step: 5000 # Consumes LOTS of time to compute (some computer takes ever)
  ckpt_step: 5000
  dump_img_step: 2000 # Consumes LOTS of disk space

  # Use this argument when there are too many micro patches
  merge_micro_patches_in_cpu: True
